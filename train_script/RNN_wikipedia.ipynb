{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cba6e1d-db3b-4902-9ba4-4195f4605896",
   "metadata": {},
   "source": [
    "# 1. Process txt data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df2511a6-4cb7-4915-8c42-3d67732efe3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import WikiCorpus\n",
    "from gensim.corpora.wikicorpus import *\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f7b6c3b-a1b1-4947-bbb2-23f42abd7c57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what', 'is', 'this', 'thing', '?']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_tokenize_re = re.compile('^([A-Za-z]|\\.|\\!|\\?\\,\\'){1,}')\n",
    "wiki_replace_re = re.compile(\"([^A-Za-z\\.\\!\\,\\?]|'s)\")\n",
    "wiki_addspace_re = re.compile('(?<! )(?=[.,!?()])|(?<=[.,!?()])(?! )')\n",
    "wiki_s_re = re.compile(\"'\")\n",
    "\n",
    "def tokenize(content):\n",
    "    content = content.lower()\n",
    "    content = wiki_replace_re.sub(' ', content)\n",
    "    content = wiki_addspace_re.sub(' ', content)\n",
    "    #conent = wiki_s_re.sub(\" '\", content)\n",
    "    # override original method in wikicorpus.py\n",
    "    return [token for token in content.split()]\n",
    "\n",
    "def process_article(args):\n",
    "   # override original method in wikicorpus.py\n",
    "    text, lemmatize, title, pageid = args\n",
    "    text = filter_wiki(text)\n",
    "    if lemmatize:\n",
    "        result = utils.lemmatize(text)\n",
    "    else:\n",
    "        result = tokenize(text)\n",
    "    return result, title, pageid\n",
    "\n",
    "\n",
    "class MyWikiCorpus(WikiCorpus):\n",
    "    def __init__(self, fname, processes=None, lemmatize=utils.has_pattern(), dictionary=None, filter_namespaces=('0',)):\n",
    "        WikiCorpus.__init__(self, fname, processes, lemmatize, dictionary, filter_namespaces)\n",
    "\n",
    "    def get_texts(self):\n",
    "        articles, articles_all = 0, 0\n",
    "        positions, positions_all = 0, 0\n",
    "        texts = ((text, self.lemmatize, title, pageid) for title, text, pageid in extract_pages(bz2.BZ2File(self.fname), self.filter_namespaces))\n",
    "        pool = multiprocessing.Pool(self.processes)\n",
    "        # process the corpus in smaller chunks of docs, because multiprocessing.Pool\n",
    "        # is dumb and would load the entire input into RAM at once...\n",
    "        for group in utils.chunkize(texts, chunksize=10 * self.processes, maxsize=1):\n",
    "            for tokens, title, pageid in pool.imap(process_article, group):  # chunksize=10):\n",
    "                articles_all += 1\n",
    "                positions_all += len(tokens)\n",
    "                # article redirects and short stubs are pruned here\n",
    "                if len(tokens) < ARTICLE_MIN_WORDS or any(title.startswith(ignore + ':') for ignore in IGNORED_NAMESPACES):\n",
    "                    continue\n",
    "                articles += 1\n",
    "                positions += len(tokens)\n",
    "                if self.metadata:\n",
    "                    yield (tokens, (pageid, title))\n",
    "                else:\n",
    "                    yield tokens\n",
    "        pool.terminate()\n",
    "\n",
    "        logger.info(\n",
    "            \"finished iterating over Wikipedia corpus of %i documents with %i positions\"\n",
    "            \" (total %i articles, %i positions before pruning articles shorter than %i words)\",\n",
    "            articles, positions, articles_all, positions_all, ARTICLE_MIN_WORDS)\n",
    "        self.length = articles  # cache corpus length\n",
    "        \n",
    "tokenize('What is this __thing_?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aaafc0ea-10de-455b-a6be-b455f8a15dd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wiki_corpus = MyWikiCorpus('enwiki-20220201-pages-articles-multistream.xml.bz2', dictionary={})\n",
    "text_num = 0\n",
    "\n",
    "if not os.path.isfile('wiki_text.txt') or 0:\n",
    "    with open('wiki_text.txt', 'w', encoding='utf-8') as f:\n",
    "        start_time = datetime.now()\n",
    "        for text in wiki_corpus.get_texts():\n",
    "            f.write(str(bytes(' '.join(text), 'utf-8').decode('utf-8') + '\\n'))\n",
    "            text_num += 1\n",
    "            if text_num % 1000 == 0:\n",
    "                use_time = datetime.now() - start_time\n",
    "                est_time = use_time*5176019/(text_num+1) - use_time\n",
    "                print('{}/5405081 articles processed. Time Used: {} Time EST.: {}'.format(text_num, use_time.__str__().split(\".\")[0], est_time.__str__().split(\".\")[0]), end='\\r')\n",
    "\n",
    "        print('{} articles processed.'.format(text_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae392a28-03a5-406d-ba6f-6b2b38e3fc30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# os.system('tail -n 1 wiki_text.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7410e5-91a3-43e8-af67-b2cac4c724e8",
   "metadata": {},
   "source": [
    "# 2. Train word to vector model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48c6616d-9d65-495a-b7da-68c74f9a6983",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7cf3c47-df33-4deb-8b53-8f8365c01b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "261be1bd-defe-463d-a6bf-d21c0198aa03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not os.path.isfile('word2vec.model') or 0: \n",
    "    sg = 0\n",
    "    window_size = 10\n",
    "    vector_size = 100\n",
    "    min_count = 100\n",
    "    workers = 16\n",
    "    epochs = 5\n",
    "    batch_words = 500000\n",
    "\n",
    "    train_data = word2vec.LineSentence('wiki_text.txt')\n",
    "    model =  word2vec.Word2Vec(\n",
    "        train_data,\n",
    "        min_count=min_count,\n",
    "        size=vector_size,\n",
    "        workers=workers,\n",
    "        window=window_size,\n",
    "        iter= epochs,\n",
    "        sg=sg,\n",
    "        batch_words=batch_words\n",
    "    )\n",
    "\n",
    "    model.save('word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6758f02a-1101-49f4-bd4f-11340aaddc51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 09:49:30,058 : INFO : loading Word2Vec object from word2vec.model\n",
      "2022-02-24 09:49:30,505 : INFO : loading wv recursively from word2vec.model.wv.* with mmap=None\n",
      "2022-02-24 09:49:30,506 : INFO : loading vectors from word2vec.model.wv.vectors.npy with mmap=None\n",
      "2022-02-24 09:49:30,528 : INFO : setting ignored attribute vectors_norm to None\n",
      "2022-02-24 09:49:30,529 : INFO : loading vocabulary recursively from word2vec.model.vocabulary.* with mmap=None\n",
      "2022-02-24 09:49:30,529 : INFO : loading trainables recursively from word2vec.model.trainables.* with mmap=None\n",
      "2022-02-24 09:49:30,530 : INFO : loading syn1neg from word2vec.model.trainables.syn1neg.npy with mmap=None\n",
      "2022-02-24 09:49:30,552 : INFO : setting ignored attribute cum_table to None\n",
      "2022-02-24 09:49:30,553 : INFO : loaded word2vec.model\n"
     ]
    }
   ],
   "source": [
    "model = word2vec.Word2Vec.load('word2vec.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482646b8-0cf5-47fb-b5e5-bae0b8c220d6",
   "metadata": {},
   "source": [
    "### Take a look at the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fe29b536-5f11-4834-b862-b5153d6c9a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('fascism', 0.8815176486968994)\n",
      "('nazism', 0.860171377658844)\n",
      "('socialism', 0.8568580746650696)\n",
      "('anarchism', 0.8435341119766235)\n",
      "('capitalism', 0.8299001455307007)\n",
      "('imperialism', 0.8271135091781616)\n",
      "('stalinism', 0.8257947564125061)\n",
      "('militarism', 0.8055943250656128)\n",
      "('zionism', 0.803575873374939)\n",
      "('nationalism', 0.8000373840332031)\n"
     ]
    }
   ],
   "source": [
    "for item in model.wv.most_similar('communism'):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5465a4d3-0c4c-4e00-9e22-2c56c85e98c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['cyberpunk'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa54ddfb-ee5d-41fd-bdfd-d3c964d34c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total distinct words: 365540\n"
     ]
    }
   ],
   "source": [
    "print('Total distinct words:',len(model.wv.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d04ea2-3e89-41bb-8efd-0381f118dbb0",
   "metadata": {},
   "source": [
    "# 2 Load train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7c3aa32-2d30-48ec-8429-d6e6bd89acac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 09:50:38,512 : INFO : Note: NumExpr detected 20 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2022-02-24 09:50:38,512 : INFO : NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1604e1-f43b-44ab-8098-3331c58ccc8c",
   "metadata": {},
   "source": [
    "### Process training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c77b8380-4b2a-4d7c-a221-492740d09c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['politic_texts/politic1.txt', 'politic_texts/politic13.txt', 'politic_texts/politic18.txt', 'politic_texts/politic9.txt', 'politic_texts/politic16.txt', 'politic_texts/politic19.txt', 'politic_texts/politic11.txt', 'politic_texts/politic14.txt', 'politic_texts/politic5.txt', 'politic_texts/politic6.txt', 'politic_texts/politic4.txt', 'politic_texts/politic15.txt', 'politic_texts/politic8.txt', 'politic_texts/politic12.txt', 'politic_texts/politic2.txt', 'politic_texts/politic10.txt', 'politic_texts/politic.txt', 'politic_texts/politic17.txt', 'politic_texts/politic7.txt', 'politic_texts/politic3.txt']\n",
      "Total words: 613223\n",
      "Total sample: 6133\n"
     ]
    }
   ],
   "source": [
    "line_length = 100\n",
    "if not os.path.isfile('politic_processed.txt') or 0:\n",
    "    words = []\n",
    "    files = os.listdir('politic_texts')\n",
    "    files = ['politic_texts/'+f for f in files if f.endswith(\".txt\")]\n",
    "    print(files)\n",
    "    for f in files:\n",
    "        with open(f, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            for line in lines:\n",
    "                words.extend(tokenize(line))\n",
    "    print('Total words:', len(words))\n",
    "    lines = []\n",
    "    while len(words)>0:\n",
    "        if len(words)>line_length:\n",
    "            lines.append(words[:line_length])\n",
    "            words = words[line_length:]\n",
    "        else:\n",
    "            lines.append(words)\n",
    "            words = []\n",
    "    print('Total sample:', len(lines))\n",
    "    with open('politic_processed.txt', 'w') as file:\n",
    "        for line in lines:\n",
    "            file.write(' '.join(line) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2a7bd1c6-e26f-465a-97db-9370e825abe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [communism, from, latin, communis, ,, common, ...\n",
       "1    [,, revolutionary, spontaneity, ,, and, worker...\n",
       "2    [this, system, there, are, two, major, social,...\n",
       "3    [in, turn, ,, establish, social, ownership, of...\n",
       "4    [s, ., criticism, of, communism, can, be, divi...\n",
       "dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('politic_processed.txt', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "train_s = pd.Series(lines)\n",
    "train_s = train_s.apply(lambda x:x.split())\n",
    "train_s.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bffe8602-7d3c-4737-819a-e5a231342c16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vec = train_s.apply(lambda x: [model.wv[word] if word in model.wv.vocab else np.zeros(model.vector_size, dtype=np.float32) for word in x])\n",
    "train_vec[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5b8c8ab9-a71d-46cf-8832-3025e5060217",
   "metadata": {},
   "outputs": [],
   "source": [
    "class wiki_politic_set(Dataset):\n",
    "    def __init__(self, sentences):\n",
    "        self.data = sentences\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = np.asarray(self.data[index])\n",
    "        y = np.append(x[1:],np.zeros(x[0].shape))\n",
    "        x = torch.tensor(x)\n",
    "        y = torch.tensor(y).reshape(x.shape)\n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    text_list, predict_list = [], []\n",
    "    for text, predict in batch:\n",
    "        text_list.append(text)\n",
    "        predict_list.append(predict)\n",
    "    text_list = pad_sequence(text_list, batch_first=True, padding_value=0)\n",
    "    predict_list = pad_sequence(predict_list, batch_first=True, padding_value=0)\n",
    "    return text_list.float(), predict_list.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c9d153a0-5149-4a92-80bc-22a14229bf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "wikiset = wiki_politic_set(train_vec)\n",
    "\n",
    "wiki_data = DataLoader(wikiset, batch_size=batch_size, collate_fn=collate_fn, sampler=RandomSampler(wikiset, replacement=True, num_samples=batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0ee8ff-17a2-4ad4-9a12-6ef3d40f0d44",
   "metadata": {},
   "source": [
    "### checkout the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a524d4f6-6e85-4362-886c-f09381bb6a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 100, 100]) torch.Size([1024, 100, 100]) \n",
      "\n",
      "of sanitary conditions and the sale of noxious drugs , the preservation of a just system of distribution these , \n",
      "sanitary conditions and the sale of noxious drugs , the preservation of a just system of distribution these , among \n"
     ]
    }
   ],
   "source": [
    "for text, pred in wiki_data:\n",
    "    print(text.shape, pred.shape, '\\n')\n",
    "    text = text[0,:,:]\n",
    "    pred = pred[0,:,:]\n",
    "    for i in range(20):\n",
    "        word = text[i].reshape(1,100)\n",
    "        word = np.asarray(word)\n",
    "        word, _ = model.wv.most_similar(positive = word, topn = 1)[0]\n",
    "        print(word, end=' ')\n",
    "    print()\n",
    "    for i in range(20):\n",
    "        word = pred[i].reshape(1,100)\n",
    "        word = np.asarray(word)\n",
    "        word, _ = model.wv.most_similar(positive = word.reshape(1,100), topn = 1)[0]\n",
    "        print(word, end=' ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361622bb-6a95-4b90-bb79-3ba843949885",
   "metadata": {},
   "source": [
    "# 3. Train RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5607a5bc-22b4-44a9-b6d6-f135f4cf555d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f66a40d3-d32d-44fa-9175-75bacc99307c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import GRU\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a0b8a0d5-8dee-4d0c-8698-b272c4c56fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "GRU = reload(GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "18b86903-5228-43c6-8a81-7e15987cb936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 5434980\n"
     ]
    }
   ],
   "source": [
    "gru_net = GRU.GRUNet().to(device)\n",
    "print('Total parameters:', sum(p.numel() for p in gru_net.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9821d0b0-6140-410b-bfaa-ca835df97022",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = .01\n",
    "optimizer = Adam(gru_net.parameters(), lr = learning_rate)\n",
    "epoches = 10000\n",
    "train_loss_history = []\n",
    "batch_num = len(wiki_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "85bc4359-19e2-4a5a-b613-42ae598cd866",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_checkpoint = False\n",
    "if load_checkpoint:\n",
    "    checkpoint = torch.load('gru.checkpoint')\n",
    "    gru_net.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    loss = checkpoint['epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "093bd976-9ad5-46b8-b302-d080443dce62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500: Train loss = 1.90405E+00 Time: 0:01:14.749593\n",
      "Epoch 1000: Train loss = 1.88765E+00 Time: 0:02:30.286432\n",
      "Epoch 1500: Train loss = 1.85220E+00 Time: 0:03:46.855830\n",
      "Epoch 2000: Train loss = 1.83416E+00 Time: 0:05:03.385470\n",
      "Epoch 2500: Train loss = 1.78540E+00 Time: 0:06:20.076761\n",
      "Epoch 3000: Train loss = 1.73598E+00 Time: 0:07:36.633291\n",
      "Epoch 3500: Train loss = 1.71617E+00 Time: 0:08:53.219149\n",
      "Epoch 4000: Train loss = 1.69063E+00 Time: 0:10:11.849992\n",
      "Epoch 4500: Train loss = 1.65828E+00 Time: 0:11:28.396448\n",
      "Epoch 5000: Train loss = 1.67557E+00 Time: 0:12:45.963534\n",
      "Epoch 5500: Train loss = 1.67496E+00 Time: 0:14:03.863957\n",
      "Epoch 6000: Train loss = 1.77757E+00 Time: 0:15:22.477537\n",
      "Epoch 6186/10000 Time Used: 0:15:52 Time EST.: 0:09:47\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3199/2584854055.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtotal_loss_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwiki_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/pyt/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/pyt/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/pyt/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/pyt/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3199/3202183793.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mappend\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/pyt/lib/python3.9/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mappend\u001b[0;34m(arr, values, axis)\u001b[0m\n\u001b[1;32m   4741\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4742\u001b[0m             \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4743\u001b[0;31m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4744\u001b[0m         \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4745\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mravel\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/pyt/lib/python3.9/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mravel\u001b[0;34m(a, order)\u001b[0m\n\u001b[1;32m   1815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1816\u001b[0m     \"\"\"\n\u001b[0;32m-> 1817\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1818\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1819\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = datetime.now()\n",
    "for epoch in range(epoches):\n",
    "    total_loss_train = []\n",
    "    \n",
    "    for x, pred in wiki_data: \n",
    "        x = x.to(device)\n",
    "        pred = pred.to(device)\n",
    "        h = gru_net.init_hidden(x.shape[0]).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output, h = gru_net(x, h)\n",
    "        loss = F.mse_loss(output, pred)\n",
    "        total_loss_train.append(loss.clone().detach().cpu())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss.detach();output.detach();x.detach();pred.detach();\n",
    "        \n",
    "    use_time = datetime.now() - start_time\n",
    "\n",
    "    est_time = use_time*epoches/(epoch+1) - use_time\n",
    "    print('Epoch {:3}/{:3} Time Used: {} Time EST.: {}'.format(\n",
    "        epoch+1, epoches, use_time.__str__().split(\".\")[0], est_time.__str__().split(\".\")[0]), end='\\r')\n",
    "    if (epoch+1)%500==0:\n",
    "        torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': gru_net.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "        'time_used': use_time,\n",
    "        }, 'gru.checkpoint.'+str(epoch+1))\n",
    "        \n",
    "        torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': gru_net.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "        'time_used': use_time,\n",
    "        }, 'gru.checkpoint')\n",
    "        \n",
    "        train_loss = np.sqrt(np.mean(total_loss_train))\n",
    "        train_loss_history.append(train_loss)\n",
    "        print(' '*50, end='\\r');\n",
    "        print('Epoch {}: Train loss = {:.5E} Time: {}'.format(epoch+1, train_loss, use_time))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7ea579-06b2-4d9f-94d1-c69dc9ff8657",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
